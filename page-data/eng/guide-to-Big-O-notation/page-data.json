{"componentChunkName":"component---src-templates-blog-post-jsx","path":"/eng/guide-to-Big-O-notation/","result":{"data":{"markdownRemark":{"html":"<p>The <strong>Big O notation</strong> is used in computer science to describe the performance or complexity of an algorithm.\nBig O specifically describes the <em>worst-case scenario</em>, and can be used to describe the time or space (e.g., in memory or on disk) required by an algorithm.</p>\n<p>Anyone who has read <em>Programming Pearls</em> or any other computer science books without a strong mathematics background might hit a wall when they reach chapters mentioning O(N log N) or other seemingly arcane syntax.</p>\n<p>This article aims to help you understand the basics of Big O and logarithms.</p>\n<p>As a programmer first and a mathematician second (or maybe third or fourth), I've found that the best way to fully grasp Big O is to create some code examples. So below are some common practices, along with descriptions and examples where possible.</p>\n<h2>O(1)</h2>\n<p>O(1) describes an algorithm that will always execute in the same time (or space) regardless of the size of the input data set.</p>\n<pre><code class=\"language-javascript\">const nums = [1, 2, 3, 4, 5];\nconst firstNumber = nums[0];\n\n</code></pre>\n<p>In our example, the input size is 5 because there are 5 elements in the array. To get the result, one operation is required (fetching an element by index). How many operations are needed if there are 100, 1000, or 100,000 elements? Only one operation is still required.</p>\n<h2>O(N)</h2>\n<p>O(N) describes an algorithm whose performance will grow linearly and in direct proportion to the size of the input data set.</p>\n<p>The example below also demonstrates how Big O supports the worst-case performance scenario.</p>\n<pre><code class=\"language-javascript\">const nums = [1, 2, 3, 4, 5];\nlet sum = 0;\nfor (let num of nums) {\n  sum += num;\n}\n</code></pre>\n<p>Again, the question is: how many input operations are required? Here, you need to iterate over each element, i.e., an operation for each element. The larger the array, the more operations.</p>\n<p>Using Big O notation: O(n), or \"complexity of order n.\" Such algorithms are also called \"linear\" or that the algorithm \"scales linearly.\"</p>\n<p>Can we make the sum more efficient? Generally no. But what if we know that the array starts with 1, is sorted, and has no gaps? Then we can apply the formula:</p>\n<img src=\"https://i.upmath.me/svg/S%20%3D%20%7Bn(n%2B1)%20%5Cover%202%7D.\" alt=\"S = {n(n+1) \\over 2}.\" />\n<p>where n is the last element of the array.</p>\n<pre><code class=\"language-javascript\">const sumContiguousArray = function (arr) {\n  //get the last item\n  const lastItem = arr[arr.length - 1];\n  //Gauss's trick\n  return (lastItem * (lastItem + 1)) / 2;\n};\nconst nums = [1, 2, 3, 4, 5];\nconst sumOfArray = sumContiguousArray(nums);\n\n</code></pre>\n<h2>O(N^2)</h2>\n<p>O(N^2) represents an algorithm whose performance is directly proportional to the square of the size of the input data set. This is typical for algorithms that include nested iterations over the data set. Deeper nested iterations will lead to O(N^3), O(N^4), etc.</p>\n<pre><code class=\"language-javascript\">const hasDuplicates = function (num) {\n  //loop the list, our O(n) op\n  for (let i = 0; i &#x3C; nums.length; i++) {\n    const thisNum = nums[i];\n    //loop the list again, the O(n^2) op\n    for (let j = 0; j &#x3C; nums.length; j++) {\n      //make sure we're not checking same number\n      if (j !== i) {\n        const otherNum = nums[j];\n        //if there's an equal value, return\n        if (otherNum === thisNum) return true;\n      }\n    }\n  }\n  //if we're here, no dups\n  return false;\n};\nconst nums = [1, 2, 3, 4, 5, 5];\nhasDuplicates(nums); //true\n</code></pre>\n<p>Iterating an array is O(N).\nBut we have a nested loop, iterating again for each element â€“ i.e., O(N^2) or \"complexity of order n square.\"</p>\n<p>Algorithms with nested loops over the same collection are always O(N^2).</p>\n<h2>O(2^N)</h2>\n<p>O(2^N) denotes an algorithm whose growth doubles with each addition to the input data set. The growth curve of an O(2^N) function is exponential - starting off shallow and then steeply rising. An example of an O(2^N) function is the recursive calculation of Fibonacci numbers:</p>\n<pre><code class=\"language-javascript\">const fibonacci = function (num) {\n  if (num &#x3C;= 1) {\n    return num;\n  }\n  return fibonacci(num - 2) + fibonacci(num - 1);\n};\nfibonacci(5); //true\n\n</code></pre>\n<h2>O(log n)</h2>\n<p><strong>Example:</strong></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Binary_search\">Binary Search</a> - this method is used for searching in sorted data sets. It works by selecting the middle element of the data set, essentially the median value, and comparing it to the target value. If the values match, it returns true. If the target value is higher than the probe element's value, it takes the upper half of the data set and performs the same operation with it. Likewise, if the target value is lower than the probe element's value, it performs the operation with the lower half. It continues to halve the data set with each iteration until the value is found or it can no longer divide the data set.</p>\n<p>This type of algorithm is described as O(log N). Iterative halving of data sets, as described in the binary search example, yields a growth curve that peaks early and slowly levels out as the size of the data sets increases, e.g., a data set of 10 elements takes one second, a data set of 100 elements takes two seconds, and a data set containing 1000 elements takes three seconds. Doubling the size of the input data set has little effect on its growth, as after one iteration of the algorithm, the data set will be halved and, therefore, on par with a data set half its size. This makes algorithms like binary search extremely efficient when working with large data sets.</p>\n<p>Such algorithms are called \"Divide and Conquer.\"</p>\n<p>In the \"binary search\" algorithm, we divide the array into two parts at each step.</p>\n<p>In the worst case, we make as many operations/divisions as we can divide the array into two parts. For example, how many times can we divide an array of 4 elements into two parts? Twice. And an array of 8 elements? Three times. So, the number of divisions/operations = log2(n) (where n is the number of elements in the array).</p>\n<p><strong>Conclusions:</strong></p>\n<ul>\n<li>Accessing an element of a collection is O(1). <em>Whether it's accessing by index in an array or by key in a dictionary, in Big O notation this is O(1)</em>.</li>\n<li>Iterating over a collection is O(n).</li>\n<li>Nested loops over the same collection are O(n^2).</li>\n<li>Divide and Conquer algorithms are always O(log n).</li>\n<li>Iterations that use Divide and Conquer are O(n log n).</li>\n</ul>","frontmatter":{"title":"Big O Notation Guide for Beginners"}}},"pageContext":{"slug":"/eng/guide-to-Big-O-notation/"}},"staticQueryHashes":["1777174035","3159585216"],"slicesMap":{}}